##  Network Policies

##  O que s√£o Network Policies?
No Kubernetes, uma Network Policy √© um conjunto de regras que definem como os Pods podem se comunicar entre si e com outros endpoints de rede. Por padr√£o, os Pods em um cluster Kubernetes podem se comunicar livremente entre si, o que pode n√£o ser ideal para todos os cen√°rios. As Network Policies permitem que voc√™ restrinja esse acesso, garantindo que apenas o tr√°fego permitido possa fluir entre os Pods ou para/endere√ßos IP externos.

## Para que Servem as Network Policies?
Network Policies s√£o usadas para:

- **Isolar** Pods de tr√°fego n√£o autorizado.
- **Controlar** o acesso √† servi√ßos espec√≠ficos.
- **Implementar**padr√µes de seguran√ßa e conformidade.

## Conceitos Fundamentais: Ingress e Egress
- **Ingress**: Regras de ingresso controlam o tr√°fego de entrada para um Pod.
- **Egress:** Regras de sa√≠da controlam o tr√°fego de sa√≠da de um Pod.
Ter o entendimento desses conceitos √© fundamental para entender como as Network Policies funcionam, pois voc√™ precisar√° especificar se uma regra se aplica ao tr√°fego de entrada ou de sa√≠da.

## Como Funcionam as Network Policies?
Network Policies utilizam SELECTORS para identificar grupos de Pods e definir regras de tr√°fego para eles. A pol√≠tica pode especificar:

- **Ingress (entrada):** quais Pods ou endere√ßos IP podem se conectar a Pods selecionados.
- **Egress (sa√≠da):** para quais Pods ou endere√ßos IP os Pods selecionados podem se conectar.
## Ainda n√£o √© padr√£o
Infelizmente, as Network Policies ainda n√£o s√£o um recurso padr√£o em todos os clusters Kubernetes. Recentemente a AWS anunciou o suporte a Network Policies no EKS, mas ainda n√£o √© um recurso padr√£o, para que voc√™ possa utilizar as Network Policies no EKS, voc√™ precisa instalar o `CNI` da AWS e depois habilitar o Network Policy nas configura√ß√µes avan√ßadas do `CNI`.

Para verificar se o seu cluster suporta Network Policies, voc√™ pode executar o seguinte comando:
```sh
kubectl api-versions | grep networking
```
Se voc√™ receber a mensagem `networking.k8s.io/v1,` significa que o seu cluster suporta Network Policies. Se voc√™ receber a mensagem `networking.k8s.io/v1beta1`, significa que o seu cluster n√£o suporta Network Policies.

Se o seu cluster n√£o suportar Network Policies, voc√™ pode utilizar o Calico para implementar as Network Policies no seu cluster. Para isso, voc√™ precisa instalar o Calico no seu cluster.

Outros `CNI` que suportam Network Policies s√£o o Weave Net e o Cilium, por exemplo.

## Criando um Cluster EKS com Network Policies
Eu acredito que nessa altura do treinamento, voc√™ j√° saiba o que √© um cluster EKS, certo?

Mas mesmo assim, eu vou fazer um pequena apresenta√ß√£o somente para resfrescar a sua mem√≥ria ou ent√£o ajudar quem est√° chegando agora por aqui. hahaha

O EKS √© o Kubernetes gerenciado pela AWS, mas o que isso significa?

Quando falamos sobre clusters Kubernetes gerenciados, estamos falando que que n√£o precisaremos nos preocupar com a instala√ß√£o e configura√ß√£o do Kubernetes, pois isso ser√° feito pela AWS. N√≥s precisaremos apenas criar o nosso cluster e gerenciar as nossas aplica√ß√µes.

Como voc√™ j√° sabe, n√≥s temos dois tipos de Nodes, os Nodes do Control Plane e os Nodes Workers. No EKS, os Nodes do Control Plane s√£o gerenciados pela AWS, ou seja, n√£o precisaremos nos preocupar com eles. J√° os Workers, n√≥s precisaremos criar e gerenciar, pelo menos na maioria dos casos.

Antes de come√ßar, vamos entender os tr√™s tipos de cluster EKS que podemos ter:

- **Managed Node Groups:** Nesse tipo de cluster, os Nodes Workers s√£o gerenciados pela AWS, ou seja, n√£o precisaremos nos preocupar com eles. A AWS ir√° criar e gerenciar os Nodes Workers para n√≥s. Esse tipo de cluster √© ideal para quem n√£o quer se preocupar com a administra√ß√£o dos Nodes Workers.
- **Self-Managed Node Groups:** Nesse tipo de cluster, os Nodes Workers s√£o gerenciados por n√≥s, ou seja, precisaremos criar e gerenciar os Nodes Workers. Esse tipo de cluster √© ideal para quem quer ter o controle total sobre os Nodes Workers.
- **Fargate:** Nesse tipo de cluster, os Nodes Workers s√£o gerenciados pela AWS, mas n√£o precisaremos nos preocupar com eles, pois a AWS ir√° criar e gerenciar os Nodes Workers para n√≥s. Esse tipo de cluster √© ideal para quem n√£o quer se preocupar com a administra√ß√£o dos Nodes Workers, mas tamb√©m n√£o quer se preocupar com a cria√ß√£o e gerenciamento dos Nodes Workers.
Evidentemente, cada tipo tem os seus pr√≥s e contras, e voc√™ precisa analisar o seu cen√°rio para escolher o tipo de cluster que melhor se encaixa nas suas necessidades.

Na maioria dos casos de ambientes produtivos, iremos optar pelo tipo Self-Managed Node Groups, pois assim teremos o controle total sobre os Nodes Workers, podendo customiza-lo e gerencia-lo da forma que acharmos melhor. Agora, se voc√™ n√£o quer se preocupar com a administra√ß√£o dos Nodes Workers, voc√™ pode optar pelo tipo Managed Node Groups ou Fargate.

Quando optamos pelo Fargate, temos que levar em considera√ß√£o que n√£o teremos acesso aos Nodes Workers, pois eles s√£o gerenciados pela AWS. Isso significa menos liberdade e recursos, mas tamb√©m significa menos preocupa√ß√£o e menos trabalho com a administra√ß√£o dos Nodes Workers.

Para o nosso exemplo, vamos escolher do tipo 'Managed Node Groups', pois assim n√£o precisaremos nos preocupar com a administra√ß√£o dos Nodes Workers, mas lembre-se que voc√™ pode escolher o tipo que melhor se encaixa nas suas necessidades.

Para criar o cluster vamos utilizar o EKSCTL, que √© uma ferramenta de linha de comando que nos ajuda a criar e gerenciar clusters EKS. Voc√™ pode encontrar mais informa√ß√µes sobre o EKSCTL aqui.

Ela acabou se tornando uma das formas oficiais de criar e gerenciar clusters EKS, e √© a ferramenta que eu mais utilizo para criar e gerenciar clusters EKS. Alias, acredito que seja a ferramenta mais utilizada para criar clusters EKS, quando n√£o estamos utilizando alguma ferramenta de IaC, como o Terraform, por exemplo.

## Instalando o EKSCTL
Para instalar o EKSCTL, voc√™ pode seguir as instru√ß√µes aqui.

Ele est√° dispon√≠vel para Linux, MacOS e Windows. Al√©m de ser poss√≠vel de rodar em um container Docker.

Em nosso exemplo, vamos utilizar o Linux, claro! hahaha

Para instalar o EKSCTL no Linux, voc√™ pode executar o seguinte comando:

# for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
```sh
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH
```
```sh
curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
```

# (Optional) Verify checksum
```sh
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check
```
```sh
tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz
```
```sh
sudo mv /tmp/eksctl /usr/local/bin
```
Eu literalmente copiei e colei o comando acima do site do EKSCTL, ent√£o n√£o tem erro. Mas lembrando, sempre √© bom consultar o site oficial para verificar se n√£o houve nenhuma altera√ß√£o.

**Aqui estamos fazendo o seguinte:**

- Definindo a arquitetura do nosso sistema, no meu caso, amd64. Voc√™ pode verificar a arquitetura do seu sistema executando o comando uname -m.
- Definindo a plataforma do nosso sistema, no meu caso, Linux_amd64.
- Baixando o bin√°rio do EKSCTL.
- Descompactando o bin√°rio do EKSCTL.
- Movendo o bin√°rio do EKSCTL para o diret√≥rio /usr/local/bin.
- Validando a instala√ß√£o do EKSCTL:
```sh
eksctl version
```
A sa√≠da deve ser algo parecido com isso:
```sh
0.177.0
```
Essa √© a vers√£o do EKSCTL que estamos utilizando no momento de cria√ß√£o desse treinamento, mas voc√™ pode estar utilizando uma vers√£o mais recente.

## Instalando o AWS CLI
Agora que temos o EKSCTL instalado, precisamos ter o AWS CLI instalado e configurado, pois o EKSCTL utiliza o AWS CLI para se comunicar com a AWS. 

O AWS CLI √© uma ferramenta de linha de comando que nos ajuda a interagir com os servi√ßos da AWS, ele √© super poderoso e √© uma das ferramentas mais utilizadas para interagir com os servi√ßos da AWS.

Vou colar abaixo os comandos para instalar o AWS CLI no Linux, mas lembre-se, sempre √© bom consultar o site oficial para verificar se n√£o houve nenhuma altera√ß√£o.
```sh
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
```
```sh
unzip awscliv2.zip
```
```sh
sudo ./aws/install
```
Validando a instala√ß√£o do AWS CLI:
```sh
aws --version
```
No meu caso, a vers√£o que estou utilizando √© a seguinte:
```sh
aws-cli/2.15.10 Python/3.11.6 Linux/6.5.0-14-generic exe/x86_64.zorin.17 prompt/off
```
Agora que temos o AWS CLI instalado, precisamos configurar o AWS CLI, para isso, voc√™ pode executar o seguinte comando:
```sh
aws configure
```
Aqui voc√™ precisa informar as suas credenciais da AWS, que voc√™ pode encontrar [aqui](https://console.aws.amazon.com/iam/home?#/securi
ty_credentials).

As informa√ß√µes que voc√™ precisa informar s√£o:

- AWS Access Key ID
- AWS Secret Access Key
- Default region name
- Default output format
O seu Access Key ID e o Secret Access Key pode ser encontrados aqui. J√° a regi√£o fica a seu critero, eu vou utilizar a regi√£o us-east-1, mas voc√™ pode utilizar a regi√£o que preferir. E por fim, o formato de sa√≠da, eu vou utilizar o formato json, mas voc√™ pode utilizar outra op√ß√£o, como text, por exemplo.

## Criando o Cluster EKS
Agora que temos o AWS CLI instalado e configurado, podemos criar o nosso cluster EKS.

Podemos cria-lo atrav√©s da linha de comando somente ou ent√£o podemos criar um arquivo de configura√ß√£o para facilitar a cria√ß√£o do cluster.

Primeiro vou trazer o comando que iremos utilizar e na sequ√™ncia vou explicar o que estamos fazendo e trazer o arquivo de configura√ß√£o.
```sh
eksctl create cluster --name=eks-cluster --version=1.28 --region=us-east-1 --nodegroup-name=eks-cluster-nodegroup --node-type=t3.medium --nodes=2 --nodes-min=1 --nodes-max=3 --managed
```
Aqui estamos fazendo o seguinte:

- eksctl create cluster: Comando para criar o cluster.
- --name: Nome do cluster.
- --version: Vers√£o do Kubernetes que iremos utilizar, no meu caso, 1.28.
- --region: Regi√£o onde o cluster ser√° criado, no meu caso, us-east-1.
- --nodegroup-name: Nome do Node Group.
- --node-type: Tipo de inst√¢ncia que iremos utilizar para os Nodes Workers, no meu caso, t3.medium.
- --nodes: Quantidade de Nodes Workers que iremos criar, no meu caso, 2.
- --nodes-min: Quantidade m√≠nima de Nodes Workers que iremos criar, no meu caso, 1.
- --nodes-max: Quantidade m√°xima de Nodes Workers que iremos criar, no meu caso, 3.
- --managed: Tipo de Node Group que iremos utilizar, no meu caso, managed.
A sa√≠da do comando deve ser algo parecido com isso:
```sh
2024-01-26 16:12:39 [‚Ñπ]  eksctl version 0.168.0
2024-01-26 16:12:39 [‚Ñπ]  using region us-east-1
2024-01-26 16:12:40 [‚Ñπ]  skipping us-east-1e from selection because it doesn't support the following instance type(s): t3.medium
2024-01-26 16:12:40 [‚Ñπ]  setting availability zones to [us-east-1c us-east-1d]
2024-01-26 16:12:40 [‚Ñπ]  subnets for us-east-1c - public:192.168.0.0/19 private:192.168.64.0/19
2024-01-26 16:12:40 [‚Ñπ]  subnets for us-east-1d - public:192.168.32.0/19 private:192.168.96.0/19
2024-01-26 16:12:40 [‚Ñπ]  nodegroup "eks-cluster-nodegroup" will use "" [AmazonLinux2/1.28]
2024-01-26 16:12:40 [‚Ñπ]  using Kubernetes version 1.28
2024-01-26 16:12:40 [‚Ñπ]  creating EKS cluster "eks-cluster" in "us-east-1" region with managed nodes
2024-01-26 16:12:40 [‚Ñπ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2024-01-26 16:12:40 [‚Ñπ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=eks-cluster'
2024-01-26 16:12:40 [‚Ñπ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "eks-cluster" in "us-east-1"
2024-01-26 16:12:40 [‚Ñπ]  CloudWatch logging will not be enabled for cluster "eks-cluster" in "us-east-1"
2024-01-26 16:12:40 [‚Ñπ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=eks-cluster'
2024-01-26 16:12:40 [‚Ñπ]  
2 sequential tasks: { create cluster control plane "eks-cluster", 
    2 sequential sub-tasks: { 
        wait for control plane to become ready,
        create managed nodegroup "eks-cluster-nodegroup",
    } 
}
2024-01-26 16:12:40 [‚Ñπ]  building cluster stack "eksctl-eks-cluster-cluster"
2024-01-26 16:12:40 [‚Ñπ]  deploying stack "eksctl-eks-cluster-cluster"
2024-01-26 16:13:10 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eks-cluster-cluster"
2024-01-26 16:13:41 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eks-cluster-cluster"
2024-01-26 16:14:41 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eks-cluster-cluster"
2024-01-26 16:24:48 [‚Ñπ]  building managed nodegroup stack "eksctl-eks-cluster-nodegroup-eks-cluster-nodegroup"
2024-01-26 16:24:49 [‚Ñπ]  deploying stack "eksctl-eks-cluster-nodegroup-eks-cluster-nodegroup"
2024-01-26 16:24:49 [‚Ñπ]  waiting for CloudFormation stack "eksctl-eks-cluster-nodegroup-eks-cluster-nodegroup"
2024-01-26 16:27:40 [‚Ñπ]  waiting for the control plane to become ready
2024-01-26 16:27:40 [‚úî]  saved kubeconfig as "/home/jeferson/.kube/config"
2024-01-26 16:27:40 [‚Ñπ]  no tasks
2024-01-26 16:27:40 [‚úî]  all EKS cluster resources for "eks-cluster" have been created
2024-01-26 16:27:41 [‚Ñπ]  nodegroup "eks-cluster-nodegroup" has 2 node(s)
2024-01-26 16:27:41 [‚Ñπ]  node "ip-192-168-55-232.ec2.internal" is ready
2024-01-26 16:27:41 [‚Ñπ]  node "ip-192-168-7-245.ec2.internal" is ready
2024-01-26 16:27:41 [‚Ñπ]  waiting for at least 1 node(s) to become ready in "eks-cluster-nodegroup"
2024-01-26 16:27:41 [‚Ñπ]  nodegroup "eks-cluster-nodegroup" has 2 node(s)
2024-01-26 16:27:41 [‚Ñπ]  node "ip-192-168-55-232.ec2.internal" is ready
2024-01-26 16:27:41 [‚Ñπ]  node "ip-192-168-7-245.ec2.internal" is ready
2024-01-26 16:27:42 [‚Ñπ]  kubectl command should work with "/home/jeferson/.kube/config", try 'kubectl get nodes'
2024-01-26 16:27:42 [‚úî]  EKS cluster "eks-cluster" in "us-east-1" region is ready
```
Pronto, cluster deployado com sucesso! üòÑ

Para vizualizar o nosso cluster, podemos executar o seguinte comando:
```sh
kubectl get nodes
```
A sa√≠da deve ser algo parecido com isso:
```sh
ip-192-168-22-217.ec2.internal   Ready    <none>   20m   v1.28.5-eks-5e0fdde
ip-192-168-50-0.ec2.internal     Ready    <none>   20m   v1.28.5-eks-5e0fdde
```
Agora vamos fazer o seguinte, vamos criar arquivo de configura√ß√£o do EKSCTL para facilitar a cria√ß√£o do cluster da pr√≥xima vez. Para isso, vamos criar um arquivo chamado eksctl.yaml e adicionar o seguinte conte√∫do:
```yaml
apiVersion: eksctl.io/v1alpha5 # Vers√£o da API do EKSCTL
kind: ClusterConfig # Tipo de recurso que estamos criando

metadata: # Metadados do recurso
  name: eks-cluster # Nome do cluster
  region: us-east-1 # Regi√£o onde o cluster ser√° criado

managedNodeGroups: # Node Groups que ser√£o criados, estamos utilizando o tipo Managed Node Groups
- name: eks-matrix-nodegroup # Nome do Node Group
  instanceType: t3.medium # Tipo de inst√¢ncia que iremos utilizar para os Nodes Workers
  desiredCapacity: 2 # Quantidade de Nodes Workers que iremos criar
  minSize: 1 # Quantidade m√≠nima de Nodes Workers que iremos criar
  maxSize: 3 # Quantidade m√°xima de Nodes Workers que iremos criar
```
Conforme voc√™ pode ver, estamos criando um cluster EKS com a mesma configura√ß√£o que utilizamos anteriormente, mas agora estamos utilizando um arquivo de configura√ß√£o para facilitar e deixar tudo mais bonitinho. üòÑ

Para criar o cluster utilizando o arquivo de configura√ß√£o, voc√™ pode executar o seguinte comando:
```sh
eksctl create cluster -f eksctl.yaml
```
A sa√≠da deve ser algo parecido com a que tivemos anteriormente, nada de novo para acrescentar aqui.

Pronto, agora que temos o nosso cluster EKS criado, vamos instalar o CNI da AWS e habilitar o Network Policy nas configura√ß√µes avan√ßadas do CNI.

## Instalando o AWS VPC CNI Plugin
O AWS VPC CNI Plugin √© um plugin de rede que permite que os Pods se comuniquem com outros Pods e servi√ßos dentro do cluster. Ele tamb√©m permite que os Pods se comuniquem com servi√ßos fora do cluster, como o Amazon S3, por exemplo.

Vamos utilizar o EKSCTL para instalar o AWS VPC CNI Plugin, para isso, voc√™ pode executar o seguinte comando:
```sh
eksctl create addon --name vpc-cni --version v1.18.0-eksbuild.1 --cluster eks-matrix --force
```
Lembrando que voc√™ precisa substituir o nome do cluster e a vers√£o do CNI pela vers√£o do seu cluster.

Voc√™ pode verificar o link abaixo para verificar a vers√£o do CNI que voc√™ precisa utilizar:

https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html

Voc√™ deve escolher a vers√£o do CNI de acordo com a vers√£o do Kubernetes que voc√™ est√° utilizando, ent√£o fique atento a isso.

Bem, voltando ao comando, o que estamos fazendo aqui √© o seguinte:

- eksctl create addon: Comando para instalar um Addon no cluster.
- --name: Nome do Addon.
- --version: Vers√£o do Addon.
- --cluster: Nome do cluster.
- --force: For√ßar a instala√ß√£o do Addon.
A sa√≠da deve ser algo parecido com isso:
```sh
2024-01-28 14:12:44 [!]  no IAM OIDC provider associated with cluster, try 'eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=eks-cluster'
2024-01-28 14:12:44 [‚Ñπ]  Kubernetes version "1.28" in use by cluster "eks-cluster"
2024-01-28 14:12:44 [!]  OIDC is disabled but policies are required/specified for this addon. Users are responsible for attaching the policies to all nodegroup roles
2024-01-28 14:12:45 [‚Ñπ]  creating addon
2024-01-28 14:13:49 [‚Ñπ]  addon "vpc-cni" active
```
Apesar de ser o CNI padr√£o do EKS, ele n√£o vem instalado por padr√£o, ent√£o precisamos instala-lo manualmente.

Caso queira visualizar os Addons instalados no seu cluster, voc√™ pode executar o seguinte comando:
```sh
eksctl get addon --cluster eks-cluster
```
A sa√≠da deve ser algo parecido com isso:
```sh
2024-01-28 14:16:44 [‚Ñπ]  Kubernetes version "1.28" in use by cluster "eks-cluster"
2024-01-28 14:16:44 [‚Ñπ]  getting all addons
2024-01-28 14:16:45 [‚Ñπ]  to see issues for an addon run `eksctl get addon --name <addon-name> --cluster <cluster-name>`
NAME	VERSION			STATUS	ISSUES	IAMROLE	UPDATE AVAILABLE	CONFIGURATION VALUES
vpc-cni	v1.16.0-eksbuild.1	ACTIVE	0		v1.16.2-eksbuild.1	
```
Ou ent√£o acessar o console da AWS e verificar os Addons instalados no seu cluster, conforme a imagem abaixo:

Alt text

Pronto, CNI instalado com sucesso! üòÑ

**Habilitando o Network Policy nas Configura√ß√µes Avan√ßadas do CNI**
Agora que temos o CNI da AWS instalado, precisamos habilitar o Network Policy nas configura√ß√µes avan√ßadas do CNI, para isso, precisamos acessar o console da AWS e seguir os seguintes passos:

- Acessar o console da AWS.
- Acessar o servi√ßo EKS.
- Selecionar o seu cluster.
- Selecionar a aba Add-ons.
- Selecionar o edit do Addon vpc-cni.
- Configura√ß√£o Avan√ßada do CNI.
- Habilitar o Network Policy.
Alt text

Depois de alguns minutos, voc√™ pode acessar o Addon vpc-cni novamente e verificar se o Network Policy est√° habilitado e atualizado com o Network Policy habilitado.

Alt text

Pronto, cluster configurado! Agora j√° podemos continuar com o nosso exemplo. üòÑ

**Instalando o Nginx Ingress Controller**
## Instalando um Nginx Ingress Controller
Para que tudo funcione bem em nosso exemplo, vamos instalar o Nginx Ingress Controller. √â importante observar a vers√£o do Ingress Controller que voc√™ est√° instalando, pois as vers√µes mais recentes ou mais antigas podem n√£o ser compat√≠veis com o Kubernetes que voc√™ est√° usando. Para este tutorial, vamos usar a`vers√£o 1.9.5.`
No seu terminal, execute os seguintes comandos:
```sh
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml
Verifique se o Ingress Controller foi instalado corretamente:
```
```sh
kubectl get pods -n ingress-nginx
```
Voc√™ pode utilizar a op√ß√£o wait do kubectl, assim quando os pods estiverem prontos, ele ir√° liberar o shell, veja:
```sh
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=90s
```
Pronto, Ingress Controller instalado com sucesso! üòÑ

## Criando Regras de Network Policy
**Ingress**
Em nosso exemplo, tanto a nossa aplica√ß√£o quanto o Redis est√£o rodando no mesmo namespace, o namespace beskar. Por padr√£o, os Pods podem se comunicar livremente entre si. Vamos criar uma Network Policy para restringir o acesso ao Redis, permitindo que somente que pods do namespace beskar possam acessar o Redis.

Para isso, vamos criar o arquivo `permitir-redis-somente-mesmo-ns.yaml` com o seguinte conte√∫do:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-allow-same-namespace
  namespace: beskar
spec:
  podSelector:
    matchLabels:
      app: redis
  ingress:
  - from:
    - podSelector: {}
```
Vamos entender o que estamos fazendo aqui:

- **apiVersion:** Vers√£o da API que estamos utilizando.
- **kind:** Tipo de recurso que estamos criando.
- **metadata:** Metadados do recurso.
   - **name:** Nome da Network Policy.
   - **namespace:** Namespace onde a Network Policy ser√° criada.
- **spec:** Especifica√ß√£o da Network Policy.
  - **podSelector:** Seletor de Pods que ser√£o afetados pela Network Policy.
    - **matchLabels:** Labels dos Pods que ser√£o afetados pela Network Policy.
   - **ingress:** Regras de entrada.
     - **from:** Origem do tr√°fego.
        - **podSelector:** Seletor de Pods que podem acessar os Pods selecionados, nesse caso, todos os Pods do namespace beskar.


Sempre que tiver o {} significa que estamos selecionando todos os Pods que atendem aos crit√©rios especificados, nesse caso, todos os Pods do namespace beskar pois n√£o estamos especificando nenhum crit√©rio.

Vamos aplicar a nossa Network Policy:
```sh
kubectl apply -f permitir-redis-somente-mesmo-ns.yaml -n beskar
```
Vamos testar o acesso ao Redis a partir de um Pod fora do namespace beskar, para isso vamos user o comando redis ping:
```sh
kubectl run -it --rm --image redis redis-client -- redis-cli -h redis-service.beskar.svc.cluster.local ping
```
Se tudo estiver funcionando corretamente, voc√™ n√£o receber√° nenhuma resposta, pois o acesso ao Redis est√° bloqueado para Pods fora do namespace beskar.

Agora, se voc√™ executar o mesmo comando, por√©m de dentro da Namespace beskar, voc√™ dever√° receber a mensagem PONG, pois o acesso ao Redis est√° permitido para Pods dentro do namespace beskar! üòÑ

Vamos testar:
```sh
kubectl run -it --rm -n beskar --image redis redis-client -- redis-cli -h redis-service.beskar.svc.cluster.local ping
```
A sa√≠da deve ser algo parecido com isso:

If you don't see a command prompt, try pressing enter.
```sh
PONG
Session ended, resume using 'kubectl attach redis-client -c redis-client -i -t' command when the pod is running
pod "redis-client" deleted
```
Pronto, agora que temos a nossa Network Policy funcionando!

Agora vamos queremos bloquear todo o acesso de entrada para os Pods do namespace beskar, para isso, vamos criar o arquivo `nao-permitir-nada-externo.yaml` com o seguinte conte√∫do:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-ns
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
```
O que mudou aqui foi o seguinte:

- **policyTypes:** Tipo de pol√≠tica que estamos criando, nesse caso, estamos criando uma pol√≠tica de entrada.
- **ingress:** Regras de entrada.
  - **from:** Origem do tr√°fego.
    - **namespaceSelector:** Seletor de Namespaces que podem acessar os Pods selecionados, nesse caso, somente o namespace beskar.
    - **matchLabels:** Labels dos Namespaces que podem acessar os Pods selecionados, nesse caso, somente o namespace beskar.
    - **kubernetes.io/metadata.name:** Nome do Namespace.
    - **beskar:** Valor do Nome do Namespace.
Simples, assim, estamos bloqueando todo o tr√°fego de entrada para os Pods do namespace `beskar,` menos para os Pods do pr√≥prio namespace beskar.

Vamos aplicar a nossa Network Policy:
```sh
kubectl apply -f nao-permitir-nada-externo.yaml -n beskar
```
Vamos testar o acesso ao Redis a partir de um Pod fora do namespace beskar, para isso vamos user o comando redis ping:
```sh
kubectl run -it --rm --image redis redis-client -- redis-cli -h redis-service.beskar.svc.cluster.local ping
```
Nada de novo, certo, por√©m vamos testar o acesso a nossa aplica√ß√£o a partir de um Pod fora do namespace beskar, para isso vamos user o comando `curl`:
```sh
kubectl run -it --rm --image curlimages/curl curl-client -- curl beskar-senhas.beskar.svc
```
Se tudo estiver funcionando corretamente, voc√™ n√£o receber√° nenhuma resposta, pois o acesso a nossa aplica√ß√£o est√° bloqueado para Pods fora do namespace beskar.

Agora, se voc√™ executar o mesmo comando, por√©m de dentro da Namespace beskar, voc√™ dever√° receber a mensagem Beskar Senhas, pois o acesso a nossa aplica√ß√£o est√° permitido para Pods dentro do namespace beskar! üòÑ

Vamos testar:
```sh
kubectl run -it --rm -n beskar --image curlimages/curl curl-client -- curl beskar-senhas.beskar.svc
```
Tudo funcionando maravilhosamente bem! De dentro do mesmo namespace, podemos acessar a nossa aplica√ß√£o e o Redis, mas de fora do namespace, n√£o podemos acessar nada! üòÑ

Mas com isso temos um problema, pois o nosso Ingress Controller n√£o consegue acessar a nossa aplica√ß√£o, pois ele est√° fora do namespace beskar, ent√£o vamos criar uma Network Policy para permitir o acesso ao nosso Ingress Controller.

Para isso, vamos criar o arquivo `permitir-ingress-controller.yaml` com o seguinte conte√∫do:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-ns-and-ingress-controller
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
```
Aqui a solu√ß√£o foi super simples, pois somente adicionamos mais um seletor de Namespaces, para permitir o acesso ao nosso Ingress Controller. Com isso, tudo que estiver dentro do namespace `ingress-nginx` e `beskar` poder√° acessar os Pods do namespace beskar.

Aqui poderiamos ter uma melhoria no c√≥digo utilizando o matchExpressions, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-ns-and-ingress-controller
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchExpressions:
        - key: kubernetes.io/metadata.name
          operator: In
          values: ["ingress-nginx", "beskar"]
```
O resultado seria o mesmo, por√©m o c√≥digo ficaria mais limpo e mais f√°cil de entender.

Agora pode testar o acesso a nossa aplica√ß√£o a partir de um Pod fora do namespace beskar, para isso vamos user o comando curl:
```sh
kubectl run -it --rm --image curlimages/curl curl-client -- curl beskar-senhas.beskar.svc
```
Aqui voc√™ n√£o conseguir√° acessar a nossa aplica√ß√£o, pois o acesso a nossa aplica√ß√£o est√° bloqueado para Pods fora do namespace beskar, agora se voc√™ executar o mesmo comando, por√©m de dentro da Namespace beskar, tudo funcionar√° bem!

Por√©m, sempre que voc√™ utilizar o endere√ßo do Ingress da nossa aplica√ß√£o, voc√™ conseguir√° normalmente, pois liberamos o acesso ao nosso Ingress Controller, portanto os clientes da nossa app que acessar√£o via internet, conseguir√£o acessar normalmente, por√©m Pods fora do namespace beskar n√£o conseguir√£o acessar a nossa aplica√ß√£o. Lindo n√£o √© mesmo? üòÑ

Somente uma observa√ß√£o importante:
```yaml
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
```
Um coisa que deve ser bem entendido no momento de criar as suas regras s√£o os operadores l√≥gicos, pois eles podem mudar completamente o resultado da sua Network Policy. No nosso exemplo, estamos utilizando o operador l√≥gico OR, ou seja, estamos permitindo o acesso ao nosso Ingress Controller OU ao nosso namespace beskar.

Se voc√™ quiser permitir o acesso ao nosso Ingress Controller E ao nosso namespace beskar, voc√™ deve utilizar o operador l√≥gico AND, veja:
```sh
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
      namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
```
Nesse caso a regra funcionar√° da seguinte forma, somente os Pods que estiverem dentro do namespace ingress-nginx E do namespace beskar poder√£o acessar os Pods do namespace beskar, ou seja, teriamos problemas.

Teste a√≠ e veja o que acontece. üòÑ

N√≥s podemos ter uma abortagem diferente, podemos ter um regra que ir√° bloquear todo o tr√°fego de entrada, e depois criar regras para permitir o tr√°fego de entrada para os Pods que precisam, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```
Agora estamos bloqueando todo o tr√°fego de entrada para os Pods do namespace beskar, pois estamos utilizando o ingress: [] para bloquear todo o tr√°fego de entrada. Mais uma vez, estamos usando o [] vazio para selecionar todos os Pods e bloquear todo o tr√°fego de entrada, pois n√£o estamos especificando nenhum crit√©rio.
O policyTypes √© um campo obrigat√≥rio, e nele voc√™ deve especificar o tipo de pol√≠tica que voc√™ est√° criando, nesse caso, estamos criando uma pol√≠tica de entrada e sa√≠da, por isso estamos utilizando o Ingress e o Egress.

Vamos aplicar:
```yaml
kubectl apply -f deny-all-ingress.yaml -n beskar
Agora vamos criar uma regra para que a nossa App consiga acessar o Redis, veja:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-redis-app-only
  namespace: beskar
spec:
  podSelector:
    matchLabels:
      app: redis
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: beskar-senhas
      namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
    ports:
    - protocol: TCP
      port: 6379
```
Com isso temos mais um regra para permitir o acesso ao Redis e a App somente entre eles e somente nas portas 6379 e 5000.

Vamos aplicar:
```sh
kubectl apply -f permitir-somente-ingress-entre-app-redis-mesmo-ns.yaml -n beskar
```
Pronto, fizemos mais uma camada de seguran√ßa, agora somente a nossa aplica√ß√£o pode acessar o Redis, e somente nas portas 6379 e 5000, mas ainda temos um problema, pois o nosso Ingress Controller n√£o consegue acessar a nossa aplica√ß√£o, e com isso, nossos clientes n√£o ir√£o conseguir acessar a nossa aplica√ß√£o, ent√£o vamos criar uma Network Policy para permitir o acesso ao nosso Ingress Controller, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-controller
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
    ports:
    - protocol: TCP
      port: 5000
```
Pronto, agora o nosso Ingress Controller consegue acessar a nossa aplica√ß√£o, e com isso, nossos clientes tamb√©m conseguem acessar a nossa aplica√ß√£o!

Mas ainda temos um problema, os nossos Pods n√£o conseguem acessar o DNS do cluster, ent√£o vamos criar uma Network Policy para permitir o acesso ao DNS do cluster e com isso o Pod de nossa App conseguir√° acessar o Redis tranquilamente
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: beskar
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
```
Pronto, um problema a menos! üòÑ
Mas ainda temos outro!

Quando criamos a regra de Egress bloqueando tudo, bloqueamos tamb√©m o tr√°fego de sa√≠da de todos os Pods do Namespace `beskar`, e com isso, o nosso Pod de App n√£o consegue acessar o Redis.
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ns
  namespace: beskar
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: beskar
      podSelector:
        matchLabels:
          app: redis
```
Pronto, agora acredito que todos os problemas foram resolvidos e podemos acessar a nossa App e o Redis normalmente! üòÑ

Outra op√ß√£o bem interessante de utilizar √© o `ipBlock`, com ele voc√™ pode especificar um endere√ßo IP ou um CIDR para permitir o acesso, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ip-block
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.18.0.0/16
```
Com a regra acima estamos falando que n√≥s queremos permitir o acesso somente para o range de IPs dentro do `CIDR 172.18.0.0/16,` ou seja, somente os Pods que estiverem dentro desse range de IPs poder√£o acessar os Pods do namespace` beskar`.

Ainda podemos adionar uma regra de exce√ß√£o, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ip-block
  namespace: beskar
spec:
  policyTypes:
  - Ingress
  podSelector: {}
  ingress:
  - from:
    - ipBlock:
        cidr: 172.18.0.0/16
        except:
        - 172.18.0.112/32
```
Com a regra acima, toda a rede `172.18.0.0/16 `ter√° acesso, menos o IP `172.18.0.112` que n√£o ter√° acesso aos Pods do namespace `beskar`.

N√≥s criamos um monte de Network Policies, mas n√£o nos concentramos em entender como ver se elas est√£o criadas e seus detalhes, ent√£o vamos ver como podemos fazer isso.

Para visualizar as Network Policies que est√£o criadas no seu cluster, voc√™ pode executar o seguinte comando:
```sh
kubectl get networkpolicies -n beskar
```
Para ver os detalhes de uma Network Policy, voc√™ pode executar o seguinte comando:
```sh
kubectl describe networkpolicy <nome-da-network-policy> -n beskar
```
Para deletar uma Network Policy, voc√™ pode executar o seguinte comando:
```sh
kubectl delete networkpolicy <nome-da-network-policy> -n beskar
```
Simples como voar, n√£o?

## Egress
N√≥s falamos muito como criar regras de Ingres, ou seja, regras de entrada, mas e as regras de sa√≠da? Como podemos criar regras de sa√≠da?

Para isso temos o `egress`, que √© muito parecido com o `ingress`, mas com algumas diferen√ßas, veja:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress
  namespace: beskar
spec:
  podSelector: {}
  policyTypes:
  - Egrees
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
```
Com a regra acima, estamos liberando o acesso para os Pods que atendem aos crit√©rios especificados, nesse caso, somente os Pods que tiverem o label app: redis poder√£o acessar os Pods do namespace beskar na porta 6379. Com isso, todos os Pods da namespace beskar poder√£o acessar os Pods que tiverem o label` app: redis` na porta 6379.

Agora, se quisermos que somente a nossa aplica√ß√£o possa acessar o Redis, podemos fazer o seguinte:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-only-app
  namespace: beskar
spec:
  policyTypes:
  - Egrees
  podSelector: 
    matchLabels:
      app: beskar-senhas
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
```
Com a regra acima, somente a nossa aplica√ß√£o poder√° acessar o Redis, pois estamos utilizando o` podSelector` para selecionar somente os Pods que tiverem o label `app: beskar-senhas`, ou seja, somente a nossa aplica√ß√£o poder√° acessar o Redis.